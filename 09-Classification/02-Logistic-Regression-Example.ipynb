{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Logistic Regression Example - Predict the Success of Bank Telemarketing\n",
        "\n",
        "Predict whether the client will subscribe a term deposit? This is a binary classification problem. The response variable, `y`, indicates if the client subscribed a term deposit and we'd like to predict it based on the historical data.\n",
        "\n",
        "Data source:\n",
        "\n",
        "[Moro et al., 2014] S. Moro, P. Cortez and P. Rita. [A Data-Driven Approach to Predict the Success of Bank Telemarketing](http://media.salford-systems.com/video/tutorial/2015/targeted_marketing.pdf). Decision Support Systems, Elsevier, 62:22-31, June 2014\n",
        "\n",
        "### Input variables\n",
        "**Bank client data:**\n",
        "1. age (numeric)\n",
        "2. job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n",
        "3. marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n",
        "4. education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n",
        "5. default: has credit in default? (categorical: 'no','yes','unknown')\n",
        "6. housing: has housing loan? (categorical: 'no','yes','unknown')\n",
        "7. loan: has personal loan? (categorical: 'no','yes','unknown')\n",
        "\n",
        "**Related with the last contact of the current campaign:**\n",
        "\n",
        "8. contact: contact communication type (categorical: 'cellular','telephone') \n",
        "9. month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n",
        "10. day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\n",
        "11. duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n",
        "\n",
        "**Other attributes:**\n",
        "\n",
        "12. campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
        "13. pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n",
        "14. previous: number of contacts performed before this campaign and for this client (numeric)\n",
        "15. poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n",
        "\n",
        "**Social and economic context attributes**\n",
        "\n",
        "16. emp_var_rate: employment variation rate - quarterly indicator (numeric)\n",
        "17. cons_price_idx: consumer price index - monthly indicator (numeric) \n",
        "18. cons_conf_idx: consumer confidence index - monthly indicator (numeric) \n",
        "19. euribor3m: euribor 3 month rate - daily indicator (numeric)\n",
        "20. nr_employed: number of employees - quarterly indicator (numeric)\n",
        "\n",
        "### Output variable (desired target)\n",
        "21. y - has the client subscribed a term deposit? (binary: 'yes','no')\n",
        "\n",
        "Importing libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# the following line gets the bucket name attached to our cluster\n",
        "bucket = spark._jsc.hadoopConfiguration().get(\"fs.gs.system.bucket\")\n",
        "\n",
        "# specifying the path to our bucket where the data is located (no need to edit this path anymore)\n",
        "data = \"gs://\" + bucket + \"/notebooks/jupyter/data/\"\n",
        "print(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Importing data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df = spark.read.format(\"csv\")\\\n",
        "  .option(\"header\", \"true\")\\\n",
        "  .option(\"inferSchema\", \"true\")\\\n",
        "  .load(data + \"bank-additional-full.csv\")\\\n",
        "  .coalesce(5)\n",
        "\n",
        "df.cache()\n",
        "df.printSchema()\n",
        "print(\"This datasets consists of {} rows.\".format(df.count()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pd.options.display.max_columns = None  # do not truncate the middle columns\n",
        "df.limit(5).toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define an RFormula that uses all of the 20 columns as features and call it `supervised`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Your answer goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fit the RFormula transformer and call it `fittedRF`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Your answer goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using `fittedRF` transform our `df` DataFrame. Call this `preparedDF`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Your answer goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Print the first couple of rows of `preparedDF`, with the truncate option off:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Your answer goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below we will retrieve the name of the columns used to make our feature vector and store them in a pandas DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "featureCols = pd.DataFrame(preparedDF.schema[\"features\"].metadata[\"ml_attr\"][\"attrs\"][\"binary\"]+\n",
        "  preparedDF.schema[\"features\"].metadata[\"ml_attr\"][\"attrs\"][\"numeric\"]).sort_values(\"idx\")\n",
        "\n",
        "featureCols = featureCols.set_index('idx')\n",
        "featureCols.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Split the transformed data into `train` and `test`. Use a 30% split and a `seed = 843`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Your answer goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Instantiate an instance of `LogisticRegression`. Call it `lr`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Your answer goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Print the parameters of `lr` to check the default values used. You can always come back to the cell above and change the default values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Your answer goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fit the model on `train` and call it `lrModel`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Your answer goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below we plot the coefficients of our model in a sorted fashion:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (8,6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "beta = np.sort(lrModel.coefficients)\n",
        "plt.plot(beta)\n",
        "plt.ylabel('Beta Coefficients')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature importance\n",
        "\n",
        "We already retrieved the name of the features. Let's join it with the coefficients to identify the ones with bigger absolute value:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "coefsArray = np.array(lrModel.coefficients)  # convert to np.array\n",
        "coefsDF = pd.DataFrame(coefsArray, columns=['coefs'])  # to pandas\n",
        "\n",
        "coefsDF = coefsDF.merge(featureCols, left_index=True, right_index=True)  # join it with featureCols we created above\n",
        "coefsDF.sort_values('coefs', inplace=True)  # Sort them\n",
        "coefsDF.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot a bar chart:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (20,3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.xticks(rotation=90)\n",
        "plt.bar(coefsDF.name, coefsDF.coefs)\n",
        "plt.title('Ranked coefficients from the logistic regression model')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From our fitted model, `lrModel`, extract the summary and call it `summary`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Your answer goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From `summary` extract `areaUnderROC`. Note that this AUC is from the `train` dataset and we should pay more attention to the AUC coming from the `test` set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Your answer goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From `summary` extract `roc` and convert it to a pandas DataFrame. Call it `roc`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Your answer goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize the `roc` DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (8,6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Your answer goes here\n",
        "roc.plot(x='FPR', y='TPR', style='-', legend=False)\n",
        "plt.title('ROC Cruve')\n",
        "plt.ylabel('TPR')\n",
        "\n",
        "print('Train AUC:', summary.areaUnderROC)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Do the same with `pr` from `summary`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Your answer goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our baseline model looks promising. Let's do some predictions on the `test` set.\n",
        "\n",
        "Pass the `test` set through our trained model. Called the resulting DataFrame `fittedTest`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Your answer goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Print the first few rows of this DataFrame. Only show the following columns: \"label\", \"prediction\", \"rawPrediction\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Your answer goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Make an evaluator from `BinaryClassificationEvaluator` function that calculates AUC. We will use this function to measure our model's performance on the `test` set. Call this evaluator `aucEvaluator`. \n",
        "\n",
        "Note that this function can be found under the `pyspark.ml.evaluation` module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Your answer goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using our `aucEvaluator` find out the AUC on the `test` set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Your answer goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Great! We see that our test and train AUC's are very close. This shows that the model we created is generalizable and would perform well on unseen data.\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "How can you improve this \"baseline\" model? We barely customized our model or features. Think about the ways you can improve the AUC. \n",
        "\n",
        "* Can you use pipeline API and create a grid search to tune the hyperparameters? \n",
        "* What are the hyperparameters that you would modify?\n",
        "* Try different regularization techniques by changing `elasticNetParam`. How does it impact our prediction power?\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "PySpark",
      "language": "python",
      "name": "pyspark"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}